Human-Machine Project Portfolio Optimization with Deep Reinforcement Learning
The robust human–machine framework enhances project portfolio selection and scheduling (PPSS) by integrating human evaluation with machine optimization, addressing several long-standing challenges associated with this complex problem [1, 2].
The framework primarily consists of two interconnected processes:
1. Machine-Assisted Project Evaluation by Humans [2, 3]:
    - Qualitative Evaluation: Instead of relying on exact numerical quantification, which is often difficult for abstract project values, this framework proposes a qualitative evaluation-based project value assessment process [1, 4]. Humans, including experts, decision-makers, and other stakeholders, provide linear qualitative evaluations (e.g., comparisons or rough estimates) rather than precise numbers [2, 3, 5].
    - Reduced Reliance on Expert Knowledge: This approach reduces the dependence on a limited number of experts and allows for broader stakeholder participation in the evaluation process [2, 6].
    - Mathematical Modeling of Uncertainty: The qualitative evaluations are translated into a set of linear equality or inequality constraints, which define a convex polytope as the feasible space for the value matrix [2, 3, 7]. This means the project value is no longer an exact matrix but an arbitrary point within this convex polygon, effectively modeling uncertainty [2, 7].
    - Machine Assistance in Evaluation: The machine assists humans by eliminating contradictory evaluations and distributing evaluation tasks efficiently [3].
    - Robust Evaluation Criteria: Given the uncertainty in project value representation, robust evaluation criteria are introduced to judge the advantages and disadvantages between two solutions [2, 8]. This involves comparing the minimum value of a convex function over the feasible domain, which occurs at the finite set of boundary points of the convex polytope, making enumeration feasible [9].
2. Preference-Based Deep Reinforcement Learning for Optimization [2, 3]:
    - Deep Preference-based Q Network (DPbQN) Algorithm: The framework introduces a novel DPbQN algorithm to compute and solve project subsets and time scheduling plans [1, 2]. This algorithm is a form of deep reinforcement learning (DRL) that leverages deep neural networks to model Q values, making it suitable for high-dimensional and large-scale combinatorial optimization problems like PPSS [10].
    - Guidance by Human Preferences: During the iterative computation, human preferences play a crucial role in guiding the optimization direction of the model [3].
    - Addressing Lack of Numerical Feedback: Unlike traditional reinforcement learning, which requires scalar target values (quantitative rewards), this preference-based learning approach is specifically tailored for situations where solutions cannot be quantitatively compared [10, 11]. Instead, training information is provided as paired comparisons expressing preferences between different objects or labels [10, 12].
    - Overcoming Traditional DRL Limitations for PPSS: While DRL has shown superior performance and wide application prospects in combinatorial optimization, it had not yet been applied to the PPSS problem [13, 14]. This framework bridges that gap.
Through these processes, the human–machine framework offers significant enhancements:
- Reduced Cost and Complexity of Project Value Assessment: The qualitative evaluation-based approach significantly lowers the cost and complexity traditionally associated with accurately assessing project value [1].
- Improved Objectivity and Reliability: It mitigates issues of subjectivity and reliability often found in traditional expert scoring methods, especially when dealing with large-scale problems and abstract project values that are resistant to quantification [6, 15].
- Higher Effectiveness and Accuracy: Experimental results demonstrate that the proposed algorithm exhibits higher effectiveness and accuracy compared to two classical algorithms (ejection chain and branch-and-price) and two heuristic algorithms (CP-ILS and DP-VNS) [1, 16]. Its advantage expands particularly as the complexity of the problem increases [16].
- Fast Solution Speeds and Strong Generalization Capabilities: The deep reinforcement learning component provides advantages in terms of fast solution speeds and strong generalization capabilities, which are crucial for many combinatorial optimization problems in various industries [13].